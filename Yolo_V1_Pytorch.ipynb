{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irissun9602/DeepLearningProject/blob/master/Yolo_V1_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZA3T8YjVczS"
      },
      "source": [
        "코드 출처 :https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLO/dataset.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su6aDynbF8Df"
      },
      "source": [
        "1.데이터 다운로드(get data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deE0BOGA0Mt4",
        "outputId": "40cc88df-419d-4a4d-c583-aa4c745e058e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-09-26 15:19:00--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
            "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
            "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460032000 (439M) [application/x-tar]\n",
            "Saving to: ‘VOCtrainval_06-Nov-2007.tar’\n",
            "\n",
            "VOCtrainval_06-Nov- 100%[===================>] 438.72M  35.4MB/s    in 13s     \n",
            "\n",
            "2022-09-26 15:19:13 (33.9 MB/s) - ‘VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxRot-qJ2g1A",
        "outputId": "94e422ad-f9a1-4838-a6c1-5d64aa4babd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-09-26 15:19:13--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
            "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
            "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 451020800 (430M) [application/x-tar]\n",
            "Saving to: ‘VOCtest_06-Nov-2007.tar’\n",
            "\n",
            "VOCtest_06-Nov-2007 100%[===================>] 430.13M  36.2MB/s    in 13s     \n",
            "\n",
            "2022-09-26 15:19:26 (33.0 MB/s) - ‘VOCtest_06-Nov-2007.tar’ saved [451020800/451020800]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYUjII-P35C4",
        "outputId": "cfa19f01-199a-4e77-c4af-26f98aa5abdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-09-26 15:19:26--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
            "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
            "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1999639040 (1.9G) [application/x-tar]\n",
            "Saving to: ‘VOCtrainval_11-May-2012.tar’\n",
            "\n",
            "VOCtrainval_11-May- 100%[===================>]   1.86G  34.3MB/s    in 55s     \n",
            "\n",
            "2022-09-26 15:20:21 (34.8 MB/s) - ‘VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YmA6nvHS3L4V"
      },
      "outputs": [],
      "source": [
        "# 압축풀기 앞에 !를 붙여줘야 한다.\n",
        "!tar xf VOCtrainval_06-Nov-2007.tar\n",
        "!tar xf VOCtest_06-Nov-2007.tar\n",
        "!tar xf VOCtrainval_11-May-2012.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ID439OwM3cn6",
        "outputId": "5df1ae68-19e0-4a19-b811-068c76e9bdcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-09-26 15:20:33--  https://pjreddie.com/media/files/voc_label.py\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2042 (2.0K) [application/octet-stream]\n",
            "Saving to: ‘voc_label.py’\n",
            "\n",
            "voc_label.py        100%[===================>]   1.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-26 15:20:34 (442 MB/s) - ‘voc_label.py’ saved [2042/2042]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# xml파일을 지우기 위해 voc_label.py를 다운받는다. 왜 xml파일을 지우는거지..?\n",
        "!wget https://pjreddie.com/media/files/voc_label.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bs24p2mu3sMi"
      },
      "outputs": [],
      "source": [
        "# 이 파일에 VOC 2012 관련 코드도 있는 듯 하다. 2007만 받아서 해보려고 하다가 2012도 추가로 받음\n",
        "!python voc_label.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QR9yfGdl4nTz"
      },
      "outputs": [],
      "source": [
        "from typing import Text\n",
        "# train, val 데이터 가져오기\n",
        "!cat 2007_train.txt 2007_val.txt 2012_*.txt > train.txt\n",
        "!cp 2007_test.txt test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEPjUmc95i-g"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm4ap7lEFlji"
      },
      "source": [
        "# 2. csv 데이터 생성(generate_csv.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r2kxijn7Llv"
      },
      "outputs": [],
      "source": [
        "#generate_csv.py\n",
        "import os\n",
        "import csv\n",
        "\n",
        "read_train = open(\"train.txt\", \"r\").readlines()\n",
        "\n",
        "with open(\"train.csv\", mode=\"w\", newline=\"\") as train_file:\n",
        "  for line in read_train:\n",
        "      image_file = line.split(\"/\")[-1].replace(\"\\n\",\"\")\n",
        "      text_file = image_file.replace(\".jpg\", \".txt\")\n",
        "      data = [image_file, text_file]\n",
        "      writer = csv.writer(train_file)\n",
        "      writer.writerow(data)\n",
        "\n",
        "read_train = open(\"test.txt\", \"r\").readlines()\n",
        "\n",
        "with open(\"test.csv\", mode=\"w\", newline=\"\") as train_file:\n",
        "    for line in read_train:\n",
        "        image_file = line.split(\"/\")[-1].replace(\"\\n\",\"\")\n",
        "        text_file = image_file.replace(\".jpg\", \".txt\")\n",
        "        data = [image_file, text_file]\n",
        "        writer = csv.writer(train_file)\n",
        "        writer.writerow(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqFj96UF5jwO"
      },
      "outputs": [],
      "source": [
        "dir_Yolo_image = '/content/gdrive/My Drive/Colab Notebooks/Yolo_V1/data/images'\n",
        "dir_Yolo_label = '/content/gdrive/My Drive/Colab Notebooks/Yolo_V1/data/labels'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGvvL1MxCNxD"
      },
      "outputs": [],
      "source": [
        "!cp VOCdevkit/*.jpg dir_Yolo_V1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BYs9a0vC-z0"
      },
      "outputs": [],
      "source": [
        "!cp VOCdevkit/VOC2007/labels/*.txt '/content/gdrive/My Drive/Colab Notebooks/Yolo_V1/data/labels'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-zBayWVETYu"
      },
      "outputs": [],
      "source": [
        "!cp VOCdevkit/VOC2012/labels/*.txt '/content/gdrive/My Drive/Colab Notebooks/Yolo_V1/data/labels'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeO1bWiS5i10"
      },
      "outputs": [],
      "source": [
        "!mv VOCdevkit/VOC2007/JPEGImages/*.jpg '/content/gdrive/My Drive/Colab Notebooks/Yolo_V1/data/images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq9CmsLkFBjh"
      },
      "outputs": [],
      "source": [
        "!mv VOCdevkit/VOC2007/labels/*.txt '/content/gdrive/My Drive/Colab Notebooks/Yolo_V1/data/labels'\n",
        "!mv VOCdevkit/VOC2012/labels/*.txt '/content/gdrive/My Drive/Colab Notebooks/Yolo_V1/data/labels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFInYDSUBhIq"
      },
      "outputs": [],
      "source": [
        "!mv VOCdevkit/VOC2012/JPEGImages/*.jpg '/content/gdrive/My Drive/Colab Notebooks/Yolo_V1/data/images'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuJA6WAeGXbV"
      },
      "source": [
        "#3. dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRFt40SJGiuA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypKI9eq7Gi1W"
      },
      "outputs": [],
      "source": [
        "class VOCDataset(torch.utils.data.Dataset):\n",
        "  def __init__(\n",
        "      self, csv_file, img_dir, label_dir, S=7, B=2, C=2, transform=None,\n",
        "  ):\n",
        "      self.annotations =pd.read_csv(csv_file)\n",
        "      self.img_dir = img_dir\n",
        "      self.label_dir = label_dir\n",
        "      self.transform = transform\n",
        "      self.S = S\n",
        "      self.B = B\n",
        "      self.C = C\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    label_path = os.path.join(self.label_dir, self.annotations.iloc[index,1])\n",
        "    boxes = []\n",
        "    with open(label_path) as f:\n",
        "      for label in f.readlines():\n",
        "        class_label, x, y, width, height = [\n",
        "            # 실수 살리기\n",
        "            float(x) if float(x) != int(float(x)) else int(x)\n",
        "            for x in label.replace(\"\\n\",\"\").split()\n",
        "        ]\n",
        "\n",
        "        boxes.append([class_label, x, y, width, height])\n",
        "\n",
        "    img_path = os.patth.join(self.img_dir, self.annotations.iloc[index,0])\n",
        "    image = Image.open(img_path)\n",
        "    boxes = torch.tensor(boxes)\n",
        "\n",
        "    if self.transform:\n",
        "      # image = self.transform(image)\n",
        "      image, boxes = self.transform(image, boxes)\n",
        "\n",
        "    # Convert To Cells (2번째 Self.S로 되어있는데 왜 두번 쓰는거지?)\n",
        "    label_matrix = torch.zeros((self.S, self.S, self.C+5 * self.B))\n",
        "    for box in boxes:\n",
        "      class_label, x, y, width, height =box.tolist()\n",
        "      class_label= int(class_label)\n",
        "\n",
        "      # i,j  repesents the cell row and cell column\n",
        "      i ,j = int(self.S * y), int(self.S * x)\n",
        "      x_cell, y_cell = self.S * x - j, self.S * y - i\n",
        "      width_cell, height_cell = (\n",
        "          width * self.S,\n",
        "          height * self.S,\n",
        "      )\n",
        "\n",
        "      if label_matrix[i, j, 20] == 0:\n",
        "        # 객체가 있을 경우\n",
        "        label_matrix[i, j, 20] = 1\n",
        "\n",
        "        # 박스 좌표\n",
        "        box_coordinates = torch.tensor(\n",
        "            [x_cell, y_cell, width_cell, height_cell]\n",
        "        )\n",
        "\n",
        "        label_matrix[i, j, 21:25] = box_coordinates\n",
        "\n",
        "        # one hot 인코딩\n",
        "        label_matrix[i, j, class_label] = 1\n",
        "    return image, label_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFdzB7qxWRTO"
      },
      "source": [
        "# 4. 유틸(utils.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db78pqR1WWWP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M99OqzSvW_mB"
      },
      "outputs": [],
      "source": [
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "  if box_format == \"midpoint\":\n",
        "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "  if box_format == \"corners\":\n",
        "    box1_x1 = boxes_preds[..., 0:1]\n",
        "    box1_y1 = boxes_preds[..., 1:2]\n",
        "    box1_x2 = boxes_preds[..., 2:3]\n",
        "    box1_y2 = boxes_preds[..., 3:4]\n",
        "    box2_x1 = boxes_labels[..., 0:1]\n",
        "    box2_y1 = boxes_labels[..., 1:2]\n",
        "    box2_x2 = boxes_labels[..., 2:3]\n",
        "    box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "  x1 = torch.max(box1_x1 , box2_x1)\n",
        "  y1 = torch.max(box1_y1, box2_y1)\n",
        "  x2 = torch.max(box1_x2 , box2_x2)\n",
        "  y2 = torch.max(box1_y2, box2_y2)\n",
        "\n",
        "  # .clamp(0) 겹치는 부분이 없을 때\n",
        "  intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "  # abs() 절댓값\n",
        "  box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "  box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "  return intersection / (box1_area + box2_area -intersection + 1e-6)\n",
        "\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "  # assert 가정문, bboxes 의 타입이 list가 아닐 때 출력문 발생\n",
        "  assert type(bboxes) == list\n",
        "\n",
        "  bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "  bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "  bboxes_after_nms = []\n",
        "\n",
        "  while bboxes:\n",
        "    chosen_box = bboxes.pop(0)\n",
        "\n",
        "    bboxes = [\n",
        "        box\n",
        "        for box in bboxes\n",
        "        if box[0] != chosen_box[0]\n",
        "        or intersection_over_union(\n",
        "            torch.tensor(chosen_box[2:1],\n",
        "            torch.tensor(box[2:]),\n",
        "            tbox_format = box_format),\n",
        "        )\n",
        "        < iou_threshold\n",
        "    ]\n",
        "\n",
        "    bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "  return bboxes_after_nms\n",
        "\n",
        "\n",
        "def mean_average_precision(\n",
        "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
        "):\n",
        "  # 각 클래스를 위한 AP 저장 리스트\n",
        "  average_precisions = []\n",
        "\n",
        "  epsilon = 1e-6\n",
        "\n",
        "  # 1개만 등록\n",
        "  for c in range(num_classes):\n",
        "    detections = []\n",
        "    ground_truths = []\n",
        "    for detection in pred_boxes :\n",
        "      if detection[1] == c:\n",
        "        detections.append(detection)\n",
        "\n",
        "    for true_box in true_boxes:\n",
        "      if true_box[1] == c:\n",
        "        ground_truths.append(true_box)\n",
        "\n",
        "    # ground_truth_box를 확인하고 리스트로 저장\n",
        "    amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "    # 텐서 형태로 변환\n",
        "    for key, val in amount_bboxes.items():\n",
        "      amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "    # 박스 확률 기준으로 정렬\n",
        "    detections.sort(key=lambda x: x[2], reverse = True)\n",
        "    TP = torch.zeros(len(detections))\n",
        "    FP = torch.zeros(len(detections))\n",
        "    total_true_bboxes = len(ground_truths)\n",
        "\n",
        "    # true_bboxes 개수가 0일 경우 스킵\n",
        "    if total_true_bboxes == 0:\n",
        "      continue\n",
        "\n",
        "    for detection_idx, detection in enumerate(detections):\n",
        "        ground_truth_img = [\n",
        "            bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "        ]\n",
        "\n",
        "        num_gts = len(ground_truth_img)\n",
        "        best_iou = 0\n",
        "\n",
        "        for idx, gt in enumerate(ground_truth_img):\n",
        "          iou = intersection_over_union(\n",
        "              torch.tensor(detection[3:]),\n",
        "              torch.tensor(gt[3:]),\n",
        "              box_format=box_format,\n",
        "          )\n",
        "\n",
        "          if iou > best_iou:\n",
        "            best_iou = iou\n",
        "            best_gt_idx = idx\n",
        "\n",
        "        if best_iou > iou_threshold:\n",
        "          if amount_bboxes[detection[0]][best_gt_idx] ==0:\n",
        "              TP[detection_idx] = 1\n",
        "              amount_bboxes[detection[0]][best_gt_idx] == 1\n",
        "          else:\n",
        "              FP[detection_idx] = 1\n",
        "\n",
        "    TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "    FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "    recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "    precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
        "    presicions = torch.cat((torch.tensor([1]), precisions))\n",
        "    recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "\n",
        "    average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "  return sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "\n",
        "def plot_image(image, boxes):\n",
        "  im = np.array(image)\n",
        "  height, width, _ = im.shpe\n",
        "\n",
        "  fig, ax = plt.subplots(1)\n",
        "  # 이미지 출력\n",
        "  ax.imshow(im)\n",
        "\n",
        "  # box[0] : 정중앙 x값, box[2] : 가로 길이\n",
        "  # box[1] : 정중앙 y값, box[3] : 세로 길이\n",
        "\n",
        "  # 직사각형 만들기\n",
        "  for box in boxes:\n",
        "    box = box[2:]\n",
        "    # 박스 안에 x,y,w,h 모두 있는지 확인하고 없으면 문구 출력\n",
        "    assert len(box) == 4, \"Got more values than in x,y,w,h in a box!\"\n",
        "    #모서리 값\n",
        "    upper_left_x = box[0] - box[2] /2\n",
        "    upper_left_y = box[1] - box[3] /2\n",
        "    rect = patches.Rectangel(\n",
        "        (upper_left_x * width, upper_left_y * height),\n",
        "        box[2] * width,\n",
        "        box[3] * height,\n",
        "        linewidth=1,\n",
        "        edgecolor=\"r\",\n",
        "        facecolor=\"none\",\n",
        "    )\n",
        "    # patch 그리기\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def get_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    iou_threshold,\n",
        "    threshold,\n",
        "    pred_format=\"cells\",\n",
        "    box_format=\"midpoint\",\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    all_pred_boxes =[]\n",
        "    all_true_boxes =[]\n",
        "\n",
        "    # 평가단계에서 사용하지 않아야 하는 layer들을 알아서 꺼주는 함수\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "\n",
        "    for batch_idx, (x, labels) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        true_bboxes = cellboxes_to_boxes(labels)\n",
        "        bboxes = cellboxes_to_boxes(predictions)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "              bboxes[idx],\n",
        "              iou_threshold=iou_threshold,\n",
        "              threshold=threshold,\n",
        "              box_format=box_format,\n",
        "            )\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "              all_pred_boxes.append([train_idx]+nms_box)\n",
        "\n",
        "            for box in true_bboxes:\n",
        "              # 대부분 확률 0으로 변환된다\n",
        "              if box[1] > threshold:\n",
        "                all_true_boxes.append([train_idx] +box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "def convert_cellboxes(predictions, S=7):\n",
        "  # 논문이랑 다르게 구현한 부분\n",
        "  # 보기가 어려워서 black box를 사용했다고 함\n",
        "\n",
        "  predictions = predictions.to(\"cpu\")\n",
        "  batch_size = predictions.shape[0]\n",
        "  predictions = predictions.reshape(batch_size, 7, 7, 30)\n",
        "  bboxes1 = predictions[..., 21:25]\n",
        "  bboxes2 = predictions[..., 26:30]\n",
        "  scores = torch.cat(\n",
        "      (predictions[...,20].unsqueeze(0), predictions[...,25].unsqueeze(0)), dim=0\n",
        "  )\n",
        "  best_box = scores.argmax(0).unsqueeze(-1)\n",
        "  best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
        "  cell_indices = torch.arrange(7).repeat(batch_size, 7,1).unsqueeze(-1)\n",
        "  x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
        "  y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0,2,1,3))\n",
        "  w_y = 1 / S * best_boxes[..., 2:4]\n",
        "  converted_bboxes = torch.cat((x,y,w_y), dim=-1)\n",
        "  predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n",
        "  best_confidence = torch.max(predictions[..., 20], predictions[...,25]).unsqueeze(-1)\n",
        "\n",
        "  converted_preds = torch.cat(\n",
        "      (predicted_class, best_confidence, converted_bboxes), dim=-1)\n",
        "\n",
        "  return converted_preds\n",
        "\n",
        "def cellboxes_to_boxes(out, S=7):\n",
        "  converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
        "  converted_pred[..., 0] = converted_pred[...,0].long()\n",
        "  all_bboxes = []\n",
        "\n",
        "  for ex_idx in range(out.shape[0]):\n",
        "    bboxes = []\n",
        "\n",
        "    for bbox_idx in range(S * S):\n",
        "      bboxes.append([x.item() for x in converted_pred[ex_idx,bbox_idx, :]])\n",
        "      all_bboxes.append(bboxes)\n",
        "\n",
        "  return all_bboxes\n",
        "\n",
        "def save_checkpoint(state,filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"=> Saving checkpoint\")\n",
        "  torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "  print(\"=> Loading checkpoint\")\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgEQ7E7-sGoG"
      },
      "source": [
        "# 5. model(model.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR0rYoovsL72"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FZMEOfPtjrf"
      },
      "outputs": [],
      "source": [
        "architecture_config = [\n",
        "    (7, 64, 2, 3),\n",
        "    \"M\",\n",
        "    (3, 192, 1, 1),\n",
        "    \"M\",\n",
        "    (1, 128, 1, 0),\n",
        "    (3, 256, 1, 1),\n",
        "    (1, 256, 1, 0),\n",
        "    (3, 512, 1, 1),\n",
        "    \"M\",\n",
        "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
        "    (1, 512, 1, 0),\n",
        "    (3, 1024, 1, 1),\n",
        "    \"M\",\n",
        "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 2, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "]\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, **kwargs):\n",
        "    super(CNNBlock, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "    self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "    self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "class Yolov1(nn.Module):\n",
        "  def __init__(self, in_channels=3, **kwargs):\n",
        "    super(Yolov1, self).__init__()\n",
        "    self.architecture = architecture_config\n",
        "    self.in_channels = in_channels\n",
        "    self.darknet = self._create_conv_layers(self.architecture)\n",
        "    self.fcs = self._create_fcs(**kwargs)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.darknet(x)\n",
        "    return self.fcs(torch.flatten(x, start_dim=1))\n",
        "\n",
        "  def _create_conv_layers(self, architecture):\n",
        "      layers = []\n",
        "      in_channels = self.in_channels\n",
        "\n",
        "      for x in architecture:\n",
        "          if type(x) == tuple:\n",
        "              layers += [\n",
        "                 CNNBlock(\n",
        "                    in_channels, x[1], kernel_size=x[0], stride=x[2],padding=x[3],\n",
        "                  )\n",
        "              ]\n",
        "              in_channels = x[1]\n",
        "          elif type(x) == str:\n",
        "            layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
        "          elif type(x) == list:\n",
        "            conv1 = x[0]\n",
        "            conv2 = x[1]\n",
        "            num_repeats = x[2]\n",
        "\n",
        "            for _ in range(num_repeats):\n",
        "                layers += [\n",
        "                    CNNBlock(\n",
        "                        in_channels,\n",
        "                        conv1[1],\n",
        "                        kernel_size=conv1[0],\n",
        "                        stride=conv1[2],\n",
        "                        padding=conv1[3],\n",
        "                    )\n",
        "                ]\n",
        "                layers += [\n",
        "                    CNNBlock(\n",
        "                        conv1[1],\n",
        "                        conv2[1],\n",
        "                        kernel_size=conv2[0],\n",
        "                        stride=conv2[2],\n",
        "                        padding=conv2[3],\n",
        "                        )\n",
        "                ]\n",
        "                in_channels = conv2[1]\n",
        "\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "  def _create_fcv(self, split_size, num_boxes, num_classes):\n",
        "      S, B, C = split_size, num_boxes, num_classes\n",
        "        # In original paper this should be\n",
        "        # nn.Linear(1024*S*S, 4096),\n",
        "        # nn.LeakyReLU(0.1),\n",
        "        # nn.Linear(4096, S*S*(B*5+C))\n",
        "\n",
        "      return nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024 * S * S, 496),\n",
        "            nn.Dropout(0.0),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(496, S * S * (C + B * 5)),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0VyZvyQxrar"
      },
      "source": [
        "# 6. 손실함수(loss.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99u-LBexx5NO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfvAkbChykhX"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(nn.Mudule):\n",
        "  def __init__(self, S=7, B=2, C=20):\n",
        "    super(YoloLoss, self).__init__()\n",
        "    self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "    # S : 이미지 자르는 크기 (7)\n",
        "    # B : 박스 개수(2)\n",
        "    # C : 클래스 개수(20)\n",
        "\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNG5m9qfZmqxZxtyfsFeuGX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}